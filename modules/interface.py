# modules/interface.py

import streamlit as st
import yaml
import os
import tempfile
import torch
import logging
from datetime import datetime
from typing import Optional, Tuple
from transformers import GPT2LMHeadModel, GPT2TokenizerFast
from .train_transformers import train_with_transformers
from .train_optuna import train_with_optuna
from .chat_predict import chat_with_model, predict_next_word
from .quantization import quantize_model
from .utils import load_config, normalize_text, load_model_and_tokenizer
from .sentiment_analysis import analyze_sentiment

# Importar para manipula√ß√£o de vari√°veis de ambiente
from dotenv import load_dotenv

# Carregar vari√°veis de ambiente do arquivo .env, se existir
load_dotenv()

# Fun√ß√£o para carregar o arquivo de configura√ß√£o
config = load_config('config/config.yaml')

# Fun√ß√£o para carregar e armazenar o modelo e tokenizador usando cache
@st.cache_resource(show_spinner=False)
def get_model_and_tokenizer(model_path: str) -> Tuple[Optional[GPT2LMHeadModel], Optional[GPT2TokenizerFast]]:
    """
    Carrega o modelo GPT-2 e o tokenizador a partir do caminho especificado.

    Args:
        model_path (str): Caminho para o diret√≥rio do modelo treinado.

    Returns:
        Tuple[Optional[GPT2LMHeadModel], Optional[GPT2TokenizerFast]]: Modelo e tokenizador carregados ou None se falhar.
    """
    model, tokenizer = load_model_and_tokenizer(model_path)
    if model is None or tokenizer is None:
        st.error("Modelo ou tokenizador n√£o carregado corretamente.")
        return None, None
    return model, tokenizer

# Configura√ß√£o da p√°gina do Streamlit
def setup_page():
    st.set_page_config(
        page_title=config['general']['app_title'],
        page_icon=config['general']['page_icon'],
        layout=config['general']['layout'],
        initial_sidebar_state=config['general']['initial_sidebar_state'],
    )

# Fun√ß√£o para aplicar CSS personalizado
def apply_custom_css():
    """
    Aplica estilos CSS personalizados para melhorar a interface do Streamlit.
    """
    custom_css = """
    <style>
    /* Fonte local "Inter" */
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap');

    html, body, [class*="css"]  {
        font-family: 'Inter', sans-serif;
        background-color: #171717;
        color: #FFFFFF;
    }

    .sidebar .sidebar-content {
        background-color: #1A1A1A;
    }

    /* Atualiza√ß√£o de seletores CSS para maior robustez */
    div[data-testid="stSidebar"] div[data-testid="stVerticalBlock"] {
        background-color: #1A1A1A;
    }

    .css-2trqyj {
        background-color: #2b2b2b;
    }

    .css-1lcbmhc {
        color: #2B4FFF;
    }

    /* Bot√µes */
    .stButton>button {
        color: #FFFFFF;
        background-color: #2B4FFF;
    }

    /* Campos de Input */
    .stTextInput>div>div>input {
        background-color: #2b2b2b;
        color: #FFFFFF;
    }

    .stFileUploader>div>div>label>div {
        background-color: #2b2b2b;
        color: #FFFFFF;
    }

    /* Tabs */
    .css-1kyxreq {
        background-color: #2b2b2b;
    }

    /* Estiliza√ß√£o de bal√µes de chat */
    .user_message {
        background-color: #3a3a3a;
        border-radius: 10px;
        padding: 10px;
        margin: 10px 0;
        text-align: right;
    }

    .bot_message {
        background-color: #2b4fff;
        border-radius: 10px;
        padding: 10px;
        margin: 10px 0;
        text-align: left;
    }

    /* Tooltips */
    .tooltip {
        position: relative;
        display: inline-block;
        cursor: pointer;
        color: #2B4FFF;
    }

    .tooltip .tooltiptext {
        visibility: hidden;
        width: 200px;
        background-color: #2b2b2b;
        color: #fff;
        text-align: left;
        border-radius: 6px;
        padding: 5px;
        position: absolute;
        z-index: 1;
        bottom: 125%; /* Position above the icon */
        left: 50%;
        margin-left: -100px;
        opacity: 0;
        transition: opacity 0.3s;
        font-size: 12px;
    }

    .tooltip:hover .tooltiptext {
        visibility: visible;
        opacity: 1;
    }

    /* Avatares */
    .avatar {
        width: 30px;
        height: 30px;
        border-radius: 50%;
        vertical-align: middle;
        margin-right: 10px;
    }

    /* Timestamps */
    .timestamp {
        font-size: 10px;
        color: #AAAAAA;
        margin-left: 5px;
    }

    </style>
    """
    st.markdown(custom_css, unsafe_allow_html=True)

# Fun√ß√£o principal para a interface do Streamlit
def run_interface():
    # Configurar a p√°gina
    setup_page()

    # Aplicar CSS personalizado
    apply_custom_css()

    # Carregar token do HuggingFace de vari√°vel de ambiente
    huggingface_token = os.getenv(config['huggingface']['token_env_var'], "")
    
    # Configura√ß√µes de Treinamento no Sidebar
    st.sidebar.header("Configura√ß√µes de Treinamento")
    
    # Guia de Uso
    with st.sidebar.expander("üìñ Guia de Uso"):
        st.markdown("""
        **GPT-2 Trainer & Chatbot**
        
        Este aplicativo permite treinar um modelo GPT-2 personalizado e interagir com ele atrav√©s de um chatbot.
        
        **Limita√ß√µes:**
        - Treinamentos longos podem levar tempo significativo, especialmente sem GPU.
        - A qualidade das respostas depende da qualidade e quantidade dos dados de treinamento.
        
        **Como Usar:**
        1. **Treinamento:**
            - Fa√ßa upload de um arquivo `.jsonl` ou `.txt` com os dados de treinamento.
            - Configure os par√¢metros de treinamento conforme necess√°rio.
            - Se desejar, ative o ajuste autom√°tico de hiperpar√¢metros com Optuna.
            - Clique em "Iniciar Treinamento" e aguarde a conclus√£o.
        2. **Chatbot:**
            - Ap√≥s o treinamento, utilize a aba "ü§ñ Chatbot" para conversar com o modelo treinado.
        3. **Prever Pr√≥xima Palavra:**
            - Utilize a aba "üîÆ Prever Pr√≥xima Palavra" para obter sugest√µes de palavras baseadas no texto fornecido.
        """)
    
    # Upload de arquivo de treinamento utilizando tempfile para gest√£o segura
    uploaded_file = st.sidebar.file_uploader(
        "Escolha o arquivo de treinamento (.jsonl ou .txt)", type=["jsonl", "txt"]
    )
    
    # Sele√ß√£o do modelo
    selected_model = st.sidebar.selectbox(
        "Escolha o modelo GPT-2",
        config['training']['model_options'],
        index=config['training']['model_options'].index(config['training']['default_model'])
    )
    
    # Inser√ß√£o do modelo personalizado do HuggingFace
    custom_model = st.sidebar.text_input(
        "Modelo Personalizado do HuggingFace",
        value=config['huggingface']['custom_model'],
        help="Insira o caminho ou nome do modelo personalizado do HuggingFace."
    )
    
    # Diret√≥rio de sa√≠da
    output_dir = st.sidebar.text_input(
        "Diret√≥rio para salvar o modelo treinado",
        value=config['training']['output_dir']
    )
    
    # Par√¢metros de treinamento com valida√ß√£o
    num_epochs = st.sidebar.number_input(
        "N√∫mero de √©pocas",
        min_value=int(1),
        max_value=int(1000),
        value=int(config['training']['num_epochs']),
        step=int(1)
    )
    logging.info(f"num_epochs type: {type(config['training']['num_epochs'])}, value: {config['training']['num_epochs']}")
    batch_size = st.sidebar.number_input(
        "Tamanho do batch",
        min_value=int(1),
        max_value=int(128),
        value=int(config['training']['batch_size']),
        step=int(1)
    )
    logging.info(f"batch_size type: {type(config['training']['batch_size'])}, value: {config['training']['batch_size']}")
    max_length = st.sidebar.number_input(
        "Comprimento m√°ximo dos tokens",
        min_value=int(16),
        max_value=int(2048),
        value=int(config['training']['max_length']),
        step=int(16)
    )
    logging.info(f"max_length type: {type(config['training']['max_length'])}, value: {config['training']['max_length']}")
    learning_rate = st.sidebar.number_input(
        "Taxa de aprendizado",
        min_value=float(1e-6),
        max_value=float(1e-3),
        value=float(config['training']['learning_rate']),
        step=float(1e-6),
        format="%.6f"
    )
    logging.info(f"learning_rate type: {type(config['training']['learning_rate'])}, value: {config['training']['learning_rate']}")
    validation_split = st.sidebar.slider(
        "Propor√ß√£o de valida√ß√£o",
        0.0,
        0.5,
        config['training']['validation_split'],
        0.05
    )
    logging.info(f"validation_split type: {type(config['training']['validation_split'])}, value: {config['training']['validation_split']}")
    
    # Gradient Accumulation Steps
    gradient_accumulation_steps = st.sidebar.number_input(
        "Gradient Accumulation Steps",
        min_value=int(1),
        max_value=int(64),
        value=int(config['training']['gradient_accumulation_steps']),
        step=int(1),
        help="N√∫mero de passos para acumula√ß√£o de gradientes antes de atualizar os pesos."
    )
    
    # Precis√£o reduzida (fp16) ou padr√£o (fp32)
    fp16_option = st.sidebar.selectbox(
        "Escolha a precis√£o do modelo",
        ["auto", "fp16", "fp32"],
        index=["auto", "fp16", "fp32"].index(config['training']['fp16_option'])
    )
    if fp16_option == "fp16":
        fp16 = True
    elif fp16_option == "fp32":
        fp16 = False
    else:
        fp16 = config['training']['fp16_option'] == "auto"
    
    # Op√ß√£o de ajuste autom√°tico de hiperpar√¢metros
    automatic_hp_tuning = st.sidebar.checkbox(
        "Ativar ajuste autom√°tico de hiperpar√¢metros (Optuna)",
        value=config['training']['automatic_hp_tuning']
    )
    
    # Op√ß√£o de estrat√©gia de avalia√ß√£o
    st.sidebar.header("Estrat√©gia de Avalia√ß√£o")
    use_eval = st.sidebar.checkbox(
        "Usar estrat√©gia de avalia√ß√£o",
        value=config['training']['use_eval_strategy']
    )
    if use_eval:
        eval_strategy = st.sidebar.selectbox(
            "Estrat√©gia de avalia√ß√£o",
            ["steps", "epoch"]
        )
        eval_steps = None
        if eval_strategy == "steps":
            eval_steps = st.sidebar.number_input(
                "N√∫mero de steps entre avalia√ß√µes",
                min_value=int(100),
                max_value=int(10000),
                value=int(config['training']['eval_steps_hf']),
                step=int(100)
            )
        # Op√ß√£o para carregar o melhor modelo ao final do treinamento
        load_best = st.sidebar.checkbox(
            "Carregar melhor modelo ao final do treinamento",
            value=config['training']['load_best_model_at_end']
        )
    else:
        eval_strategy = None
        eval_steps = None
        load_best = False  # N√£o carregar o melhor modelo se n√£o houver avalia√ß√£o
    
    # Bot√£o para iniciar o treinamento
    if st.sidebar.button("Iniciar Treinamento"):
        if uploaded_file is None:
            st.sidebar.error("Por favor, fa√ßa upload de um arquivo de treinamento.")
        else:
            # Validar par√¢metros de entrada
            if not (0 <= validation_split <= 0.5):
                st.sidebar.error("A propor√ß√£o de valida√ß√£o deve estar entre 0 e 0.5.")
            elif not (1e-6 <= learning_rate <= 1e-3):
                st.sidebar.error("A taxa de aprendizado deve estar entre 1e-6 e 1e-3.")
            elif not (16 <= max_length <= 1024):
                st.sidebar.error("O comprimento m√°ximo dos tokens deve estar entre 16 e 1024.")
            else:
                # Utilizar tempfile para gest√£o segura de arquivos tempor√°rios
                with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[1]) as temp_file:
                    temp_file.write(uploaded_file.getbuffer())
                    data_path = temp_file.name
                
                # Barra de progresso (apenas para HuggingFace Trainer)
                progress_bar = st.sidebar.progress(0) if use_eval and eval_strategy == "steps" else None
                status_text = st.sidebar.empty()
                
                # Iniciar o treinamento com tratamento de erros
                try:
                    with st.spinner("Iniciando o treinamento..."):
                        if automatic_hp_tuning:
                            # Usar PyTorch Lightning com Optuna
                            train_with_optuna(
                                data_path=data_path,
                                model_name=selected_model,
                                output_dir=output_dir,
                                num_epochs=num_epochs,
                                batch_size=batch_size,
                                max_length=max_length,
                                learning_rate=learning_rate,
                                validation_split=validation_split,
                                fp16=fp16,
                                metric_for_best_model=config['training']['metric_for_best_model'],
                                automatic_hp_tuning=automatic_hp_tuning,
                                use_eval_strategy=use_eval,
                                eval_strategy=eval_strategy,
                                eval_steps=eval_steps,
                                load_best_model_at_end=load_best,
                                huggingface_token=huggingface_token,
                                custom_model=custom_model
                            )
                        else:
                            # Usar HuggingFace Trainer
                            train_with_transformers(
                                data_path=data_path,
                                model_name=selected_model,
                                output_dir=output_dir,
                                num_epochs=num_epochs,
                                batch_size=batch_size,
                                max_length=max_length,
                                learning_rate=learning_rate,
                                validation_split=validation_split,
                                save_steps=config['training']['save_steps'],
                                logging_steps=config['training']['logging_steps'],
                                save_total_limit=config['training']['save_total_limit'],
                                fp16=fp16,
                                load_best_model_at_end=use_eval,  # Altera√ß√£o importante
                                metric_for_best_model=config['training']['metric_for_best_model'],
                                gradient_accumulation_steps=gradient_accumulation_steps,
                                use_eval_strategy=use_eval,
                                eval_strategy=eval_strategy,
                                eval_steps_hf=eval_steps if eval_strategy == "steps" else None,
                                progress_bar=progress_bar,
                                huggingface_token=huggingface_token,
                                custom_model=custom_model
                            )
                    
                    st.sidebar.success("Treinamento conclu√≠do com sucesso!")
                    
                    # Limpar o cache para recarregar o modelo treinado
                    get_model_and_tokenizer.clear()

                    # Verificar se todos os arquivos do modelo foram salvos corretamente
                    model_files = ['config.json', 'vocab.json', 'merges.txt', 'tokenizer.json']
                    if not all(os.path.exists(os.path.join(output_dir, f)) for f in model_files):
                        st.error(f"O modelo no diret√≥rio '{output_dir}' est√° incompleto. Arquivos faltantes: {[f for f in model_files if not os.path.exists(os.path.join(output_dir, f))]}")
                    else:
                        st.sidebar.success("Modelo salvo corretamente!")
                    
                    # Carregar o melhor modelo ou o √∫ltimo modelo salvo com base na estrat√©gia de avalia√ß√£o
                    if use_eval:
                        st.sidebar.info("Carregando o melhor modelo treinado com base na m√©trica.")
                        # L√≥gica para carregar o melhor modelo com base na m√©trica
                    else:
                        st.sidebar.info("Carregando o √∫ltimo modelo treinado.")
                        # L√≥gica para carregar o √∫ltimo modelo salvo

                    # Reiniciar a aplica√ß√£o para atualizar a interface
                    st.experimental_rerun()
                    
                except Exception as e:
                    st.sidebar.error(f"Ocorreu um erro durante o treinamento: {e}")
                finally:
                    # Remover o arquivo tempor√°rio ap√≥s o treinamento
                    if os.path.exists(data_path):
                        os.remove(data_path)
    
    # Abas para Chat e Previs√£o
    tab1, tab2 = st.tabs(["ü§ñ Chatbot", "üîÆ Prever Pr√≥xima Palavra"])
    
    with tab1:
        st.header("Chat com o Modelo GPT-2")
        model_path = output_dir if os.path.exists(output_dir) else config['training']['output_dir']
        
        # Verificar se o modelo est√° completo antes de prosseguir
        model_files = ['config.json', 'vocab.json', 'merges.txt', 'tokenizer.json']
        if not all(os.path.exists(os.path.join(model_path, f)) for f in model_files):
            st.error(f"O modelo no diret√≥rio '{model_path}' n√£o est√° completo ou n√£o foi treinado.")
            st.stop()  # Parar a execu√ß√£o da aba se o modelo n√£o estiver dispon√≠vel
        else:
            # Carregar modelo e tokenizador com cache
            model, tokenizer = get_model_and_tokenizer(model_path)
            if model is None or tokenizer is None:
                st.error("Falha ao carregar o modelo ou tokenizador.")
                st.stop()
            else:
                device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
                model.to(device)
    
                # Hist√≥rico de conversas armazenado no Session State
                if 'chat_history' not in st.session_state:
                    st.session_state['chat_history'] = []
    
                # Exibir hist√≥rico de mensagens
                for chat in st.session_state['chat_history']:
                    if chat['role'] == 'user':
                        st.markdown(f"""
                        <div>
                            <img src="https://i.imgur.com/7yUvePI.png" class="avatar">
                            <span class="user_message">{chat['content']}</span>
                            <span class="timestamp">{chat['timestamp']}</span>
                        </div>
                        """, unsafe_allow_html=True)
                    else:
                        st.markdown(f"""
                        <div>
                            <img src="https://i.imgur.com/6RMhx.gif" class="avatar">
                            <span class="bot_message">{chat['content']}</span>
                            <span class="timestamp">{chat['timestamp']}</span>
                        </div>
                        """, unsafe_allow_html=True)
    
                # Par√¢metros com tooltips
                col1, col2, col3, col4 = st.columns([1, 1, 1, 0.2])
                with col1:
                    temperature = st.slider("Temperature", 0.1, 1.5, config['chatbot']['temperature'], 0.1)
                with col2:
                    top_k = st.slider("Top K", 10, 100, config['chatbot']['top_k'], 10)
                with col3:
                    top_p = st.slider("Top P", 0.5, 1.0, config['chatbot']['top_p'], 0.05)
                with col4:
                    st.markdown("""
                    <div class="tooltip">?
                        <span class="tooltiptext">Controla a diversidade da gera√ß√£o. Valores mais altos resultam em respostas mais variadas.</span>
                    </div>
                    """, unsafe_allow_html=True)
    
                with col1:
                    penalty = st.slider("Repetition Penalty", 1.0, 2.0, config['chatbot']['penalty'], 0.1)
                with col2:
                    debug_mode = st.checkbox("Modo de Depura√ß√£o", value=config['chatbot']['debug_mode'])
                with col3:
                    language = st.selectbox("Idioma", ["Portugu√™s", "Ingl√™s", "Espanhol"], index=0)
                with col4:
                    pass  # Espa√ßo para alinhamento
    
                # Entrada do usu√°rio
                user_input = st.text_input("Voc√™:", key="chat_input")
    
                # Bot√£o desabilitado quando o campo est√° vazio
                send_button = st.button("Enviar", disabled=not user_input.strip())
    
                # Bot√£o para limpar o hist√≥rico da conversa
                clear_button = st.button("Limpar Conversa")
    
                if clear_button:
                    st.session_state['chat_history'] = []
                    # N√£o usar st.experimental_rerun para evitar loops
                    st.experimental_rerun()
    
                if send_button:
                    if not user_input.strip():
                        st.warning("Por favor, insira um texto para conversar.")
                    else:
                        try:
                            with st.spinner("O bot est√° pensando..."):
                                normalized_input = normalize_text(user_input)
    
                                # An√°lise de sentimentos
                                sentiment = analyze_sentiment(normalized_input, device=0 if device.type == 'cuda' else -1)
    
                                # Ajuste da resposta com base no sentimento
                                if sentiment == "NEGATIVE":
                                    prompt = f"{normalized_input}\nResponda de forma reconfortante."
                                else:
                                    prompt = normalized_input
    
                                response, debug_info = chat_with_model(
                                    prompt,
                                    model_path=model_path,
                                    temperature=temperature,
                                    top_k=top_k,
                                    top_p=top_p,
                                    penalty=penalty,
                                    debug_mode=debug_mode
                                )
    
                                # Adicionar ao hist√≥rico
                                st.session_state['chat_history'].append({
                                    'role': 'user',
                                    'content': normalized_input,
                                    'timestamp': datetime.now().strftime("%H:%M:%S")
                                })
                                st.session_state['chat_history'].append({
                                    'role': 'bot',
                                    'content': response,
                                    'timestamp': datetime.now().strftime("%H:%M:%S")
                                })
    
                                # Feedback para aprendizado cont√≠nuo
                                if 'continuous_learning' not in st.session_state:
                                    st.session_state['continuous_learning'] = []
                                st.session_state['continuous_learning'].append({
                                    'input': normalized_input,
                                    'response': response
                                })
    
                                # Mostrar debug info se ativado
                                if debug_info:
                                    st.write("**Debug Info:**")
                                    st.json(debug_info)
    
                        except Exception as e:
                            st.error(f"Ocorreu um erro ao gerar a resposta: {e}")

    with tab2:
        st.header("Prever Pr√≥xima Palavra")
        model_path = output_dir if os.path.exists(output_dir) else config['training']['output_dir']
        
        # Verificar se o modelo est√° completo antes de prosseguir
        model_files = ['config.json', 'vocab.json', 'merges.txt', 'tokenizer.json']
        if not all(os.path.exists(os.path.join(model_path, f)) for f in model_files):
            st.error(f"O modelo no diret√≥rio '{model_path}' n√£o est√° completo ou n√£o foi treinado.")
            st.stop()
        else:
            # Carregar modelo e tokenizador com cache
            model, tokenizer = get_model_and_tokenizer(model_path)
            if model is None or tokenizer is None:
                st.error("Falha ao carregar o modelo ou tokenizador.")
                st.stop()
            else:
                device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
                model.to(device)
    
                # Entrada do usu√°rio
                input_text = st.text_input("Insira o texto para prever a pr√≥xima palavra:", key="predict_input")
    
                # Bot√£o para prever
                predict_button = st.button("Prever Pr√≥xima Palavra", disabled=not input_text.strip())
    
                if predict_button:
                    if not input_text.strip():
                        st.warning("Por favor, insira um texto para prever a pr√≥xima palavra.")
                    else:
                        try:
                            with st.spinner("Prevendo a pr√≥xima palavra..."):
                                next_word = predict_next_word(
                                    input_text,
                                    model_path=model_path,
                                    device=device
                                )
                                st.success(f"Pr√≥xima palavra prevista: **{next_word}**")
                        except Exception as e:
                            st.error(f"Ocorreu um erro ao prever a pr√≥xima palavra: {e}")

# Executar a interface
run_interface()
